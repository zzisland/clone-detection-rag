#!/usr/bin/env python3
"""
RAG ç³»ç»Ÿè¯„ä¼°è„šæœ¬
è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å¼•ç”¨F1ã€å¹»è§‰ç‡ã€å“åº”æ—¶é—´
"""

import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

sys.path.insert(0, str(Path(__file__).parent / "src"))

from rag import CloneDetectionRAG
from retriever import RetrieverManager

class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, model_size="1.5B"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        print("=" * 80)
        print("RAG ç³»ç»Ÿè¯„ä¼°å·¥å…·")
        print("=" * 80)
        
        print(f"\n[1/2] åˆå§‹åŒ– RAG ç³»ç»Ÿï¼ˆæ¨¡å‹: {model_size}ï¼‰...")
        self.rag = CloneDetectionRAG(model_size=model_size)
        
        print("\n[2/2] åŠ è½½æµ‹è¯•æ•°æ®é›†...")
        self.test_data = self._load_test_data()
        print(f"âœ… åŠ è½½äº† {len(self.test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        self.results = []
    
    def _load_test_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        # æµ‹è¯•é—®é¢˜é›†ï¼ˆæ¶µç›–ä¸åŒç±»å‹ï¼‰
        test_questions = [
            # åŸºç¡€æ¦‚å¿µç±»ï¼ˆ10ä¸ªï¼‰
            {
                "question": "ä»€ä¹ˆæ˜¯ä»£ç å…‹éš†æ£€æµ‹ï¼Ÿ",
                "expected_keywords": ["ä»£ç å…‹éš†", "ç›¸ä¼¼", "é‡å¤", "ä»£ç ç‰‡æ®µ"],
                "category": "concept",
                "difficulty": "easy"
            },
            {
                "question": "Type-1å…‹éš†æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["Type-1", "å®Œå…¨ç›¸åŒ", "ç©ºæ ¼", "æ³¨é‡Š"],
                "category": "concept",
                "difficulty": "easy"
            },
            {
                "question": "Type-2å…‹éš†å’ŒType-1å…‹éš†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "expected_keywords": ["Type-2", "æ ‡è¯†ç¬¦", "å˜é‡å", "ç±»å‹"],
                "category": "concept",
                "difficulty": "medium"
            },
            {
                "question": "Type-3å…‹éš†çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["Type-3", "è¯­å¥", "ä¿®æ”¹", "æ·»åŠ ", "åˆ é™¤"],
                "category": "concept",
                "difficulty": "medium"
            },
            {
                "question": "Type-4å…‹éš†å¦‚ä½•å®šä¹‰ï¼Ÿ",
                "expected_keywords": ["Type-4", "åŠŸèƒ½", "è¯­ä¹‰", "å®ç°æ–¹å¼"],
                "category": "concept",
                "difficulty": "hard"
            },
            {
                "question": "ä»€ä¹ˆæ˜¯ASTæ–¹æ³•ï¼Ÿ",
                "expected_keywords": ["AST", "æŠ½è±¡è¯­æ³•æ ‘", "è¯­æ³•ç»“æ„"],
                "category": "concept",
                "difficulty": "medium"
            },
            {
                "question": "Tokenæ–¹æ³•çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["Token", "è¯æ³•", "åºåˆ—", "åŒ¹é…"],
                "category": "concept",
                "difficulty": "medium"
            },
            {
                "question": "ä»€ä¹ˆæ˜¯PDGæ–¹æ³•ï¼Ÿ",
                "expected_keywords": ["PDG", "ç¨‹åºä¾èµ–å›¾", "æ§åˆ¶æµ", "æ•°æ®æµ"],
                "category": "concept",
                "difficulty": "hard"
            },
            {
                "question": "ä»£ç å…‹éš†æ£€æµ‹æœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ",
                "expected_keywords": ["é‡æ„", "ç»´æŠ¤", "è´¨é‡", "ç‰ˆæƒ"],
                "category": "concept",
                "difficulty": "easy"
            },
            {
                "question": "ä»£ç å…‹éš†æ£€æµ‹é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ",
                "expected_keywords": ["å‡†ç¡®ç‡", "å¬å›ç‡", "æ€§èƒ½", "å¯æ‰©å±•æ€§"],
                "category": "concept",
                "difficulty": "medium"
            },
            
            # å·¥å…·æ¯”è¾ƒç±»ï¼ˆ5ä¸ªï¼‰
            {
                "question": "NiCadå·¥å…·çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["NiCad", "Type-1", "Type-2", "Type-3"],
                "category": "tool",
                "difficulty": "medium"
            },
            {
                "question": "CCFinderå’ŒNiCadæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "expected_keywords": ["CCFinder", "NiCad", "Token", "AST"],
                "category": "tool",
                "difficulty": "medium"
            },
            {
                "question": "SourcererCCçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["SourcererCC", "å¤§è§„æ¨¡", "å¯æ‰©å±•", "æ€§èƒ½"],
                "category": "tool",
                "difficulty": "medium"
            },
            {
                "question": "å“ªä¸ªå·¥å…·é€‚åˆæ£€æµ‹Type-4å…‹éš†ï¼Ÿ",
                "expected_keywords": ["Type-4", "è¯­ä¹‰", "åŠŸèƒ½"],
                "category": "tool",
                "difficulty": "hard"
            },
            {
                "question": "å¼€æºå…‹éš†æ£€æµ‹å·¥å…·æœ‰å“ªäº›ï¼Ÿ",
                "expected_keywords": ["NiCad", "CCFinder", "SourcererCC", "JPlag"],
                "category": "tool",
                "difficulty": "easy"
            },
            
            # æŠ€æœ¯ç»†èŠ‚ç±»ï¼ˆ5ä¸ªï¼‰
            {
                "question": "å¦‚ä½•è¯„ä¼°å…‹éš†æ£€æµ‹å·¥å…·çš„æ€§èƒ½ï¼Ÿ",
                "expected_keywords": ["å‡†ç¡®ç‡", "å¬å›ç‡", "F1", "ç²¾ç¡®ç‡"],
                "category": "technical",
                "difficulty": "medium"
            },
            {
                "question": "ä»€ä¹ˆæ˜¯å…‹éš†å¯¹ï¼Ÿ",
                "expected_keywords": ["å…‹éš†å¯¹", "ä»£ç ç‰‡æ®µ", "ç›¸ä¼¼"],
                "category": "technical",
                "difficulty": "easy"
            },
            {
                "question": "ä»€ä¹ˆæ˜¯å…‹éš†ç±»ï¼Ÿ",
                "expected_keywords": ["å…‹éš†ç±»", "ç­‰ä»·ç±»", "ç›¸ä¼¼ä»£ç "],
                "category": "technical",
                "difficulty": "medium"
            },
            {
                "question": "å¦‚ä½•å¤„ç†å¤§è§„æ¨¡ä»£ç åº“çš„å…‹éš†æ£€æµ‹ï¼Ÿ",
                "expected_keywords": ["ç´¢å¼•", "åˆ†å¸ƒå¼", "å¹¶è¡Œ", "ä¼˜åŒ–"],
                "category": "technical",
                "difficulty": "hard"
            },
            {
                "question": "å…‹éš†æ£€æµ‹çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ",
                "expected_keywords": ["å¤æ‚åº¦", "O(n", "æ€§èƒ½"],
                "category": "technical",
                "difficulty": "hard"
            },
            
            # ä¸ç¡®å®šé—®é¢˜ï¼ˆåº”è¯¥æ‹’ç»å›ç­”ï¼‰
            {
                "question": "æ˜å¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                "expected_keywords": [],
                "category": "uncertain",
                "difficulty": "n/a",
                "should_refuse": True
            },
            {
                "question": "å¦‚ä½•åšçº¢çƒ§è‚‰ï¼Ÿ",
                "expected_keywords": [],
                "category": "uncertain",
                "difficulty": "n/a",
                "should_refuse": True
            }
        ]
        
        return test_questions
    
    def evaluate_answer_quality(self, question: str, answer: str, expected_keywords: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        # 1. å…³é”®è¯è¦†ç›–ç‡
        answer_lower = answer.lower()
        matched_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]
        keyword_coverage = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0
        
        # 2. å›ç­”é•¿åº¦ï¼ˆåˆç†æ€§ï¼‰
        answer_length = len(answer)
        length_score = 1.0 if 50 <= answer_length <= 1000 else 0.5
        
        # 3. æ˜¯å¦åŒ…å«"ä¸ç¡®å®š"ã€"ä¸çŸ¥é“"ç­‰æ‹’ç»è¯
        refuse_keywords = ["ä¸ç¡®å®š", "ä¸çŸ¥é“", "æ— æ³•å›ç­”", "æŠ±æ­‰", "æ²¡æœ‰æ‰¾åˆ°"]
        has_refuse = any(kw in answer for kw in refuse_keywords)
        
        return {
            "keyword_coverage": keyword_coverage,
            "matched_keywords": matched_keywords,
            "length_score": length_score,
            "answer_length": answer_length,
            "has_refuse": has_refuse
        }
    
    def evaluate_citation(self, sources: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°å¼•ç”¨è´¨é‡"""
        # 1. æ˜¯å¦æœ‰å¼•ç”¨
        has_citation = len(sources) > 0
        
        # 2. å¼•ç”¨æ•°é‡
        citation_count = len(sources)
        
        # 3. å¼•ç”¨å¤šæ ·æ€§ï¼ˆä¸åŒæ¥æºï¼‰
        unique_sources = len(set(sources))
        diversity = unique_sources / citation_count if citation_count > 0 else 0
        
        return {
            "has_citation": has_citation,
            "citation_count": citation_count,
            "unique_sources": unique_sources,
            "diversity": diversity
        }
    
    def detect_hallucination(self, answer: str, sources: List[str]) -> bool:
        """æ£€æµ‹å¹»è§‰ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å¦‚æœæ²¡æœ‰å¼•ç”¨ä½†ç»™å‡ºäº†è¯¦ç»†å›ç­”ï¼Œå¯èƒ½æ˜¯å¹»è§‰
        if len(sources) == 0 and len(answer) > 100:
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯´æ˜äº†æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
            refuse_keywords = ["æ²¡æœ‰æ‰¾åˆ°", "æ— æ³•", "ä¸ç¡®å®š"]
            if not any(kw in answer for kw in refuse_keywords):
                return True
        return False
    
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\n" + "=" * 80)
        print("å¼€å§‹è¯„ä¼°")
        print("=" * 80)
        
        total_questions = len(self.test_data)
        
        for idx, test_case in enumerate(self.test_data, 1):
            question = test_case["question"]
            expected_keywords = test_case["expected_keywords"]
            category = test_case["category"]
            difficulty = test_case["difficulty"]
            should_refuse = test_case.get("should_refuse", False)
            
            print(f"\n[{idx}/{total_questions}] æµ‹è¯•é—®é¢˜: {question}")
            print(f"   ç±»åˆ«: {category} | éš¾åº¦: {difficulty}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            try:
                # è·å–å›ç­”
                result = self.rag.get_chat_response(question)
                answer = result.get("answer", "")
                sources = result.get("sources", [])
                confidence = result.get("confidence", "medium")
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - start_time
                
                # è¯„ä¼°å›ç­”è´¨é‡
                quality_metrics = self.evaluate_answer_quality(question, answer, expected_keywords)
                
                # è¯„ä¼°å¼•ç”¨è´¨é‡
                citation_metrics = self.evaluate_citation(sources)
                
                # æ£€æµ‹å¹»è§‰
                has_hallucination = self.detect_hallucination(answer, sources)
                
                # è¯„ä¼°æ˜¯å¦æ­£ç¡®æ‹’ç»
                correct_refuse = should_refuse and quality_metrics["has_refuse"]
                incorrect_refuse = not should_refuse and quality_metrics["has_refuse"]
                
                # è®¡ç®—ç»¼åˆå¾—åˆ†
                if should_refuse:
                    # å¯¹äºä¸ç¡®å®šé—®é¢˜ï¼Œåº”è¯¥æ‹’ç»å›ç­”
                    score = 1.0 if correct_refuse else 0.0
                else:
                    # å¯¹äºæ­£å¸¸é—®é¢˜ï¼Œç»¼åˆè¯„åˆ†
                    score = (
                        quality_metrics["keyword_coverage"] * 0.4 +
                        quality_metrics["length_score"] * 0.2 +
                        (1.0 if citation_metrics["has_citation"] else 0.0) * 0.3 +
                        (0.0 if has_hallucination else 1.0) * 0.1
                    )
                
                # ä¿å­˜ç»“æœ
                result_data = {
                    "question": question,
                    "answer": answer,
                    "sources": sources,
                    "confidence": confidence,
                    "category": category,
                    "difficulty": difficulty,
                    "should_refuse": should_refuse,
                    "response_time": response_time,
                    "score": score,
                    "quality_metrics": quality_metrics,
                    "citation_metrics": citation_metrics,
                    "has_hallucination": has_hallucination,
                    "correct_refuse": correct_refuse,
                    "incorrect_refuse": incorrect_refuse
                }
                
                self.results.append(result_data)
                
                # æ˜¾ç¤ºç»“æœ
                print(f"   âœ… å¾—åˆ†: {score:.2f}")
                print(f"   å…³é”®è¯è¦†ç›–: {quality_metrics['keyword_coverage']:.2%}")
                print(f"   å¼•ç”¨æ•°é‡: {citation_metrics['citation_count']}")
                print(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                if has_hallucination:
                    print(f"   âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰")
                
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
                self.results.append({
                    "question": question,
                    "error": str(e),
                    "score": 0.0
                })
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("è¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)
        
        # è¿‡æ»¤æ‰é”™è¯¯çš„ç»“æœ
        valid_results = [r for r in self.results if "error" not in r]
        
        if not valid_results:
            print("\nâŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
            return
        
        # 1. æ€»ä½“æŒ‡æ ‡
        print("\nğŸ“Š æ€»ä½“æŒ‡æ ‡:")
        print("-" * 80)
        
        total_score = np.mean([r["score"] for r in valid_results])
        print(f"  å¹³å‡å¾—åˆ†: {total_score:.2%}")
        
        avg_response_time = np.mean([r["response_time"] for r in valid_results])
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’")
        
        # 2. å‡†ç¡®ç‡ï¼ˆå…³é”®è¯è¦†ç›–ç‡ï¼‰
        keyword_coverages = [r["quality_metrics"]["keyword_coverage"] for r in valid_results if not r.get("should_refuse", False)]
        if keyword_coverages:
            avg_accuracy = np.mean(keyword_coverages)
            print(f"  å¹³å‡å‡†ç¡®ç‡ï¼ˆå…³é”®è¯è¦†ç›–ï¼‰: {avg_accuracy:.2%}")
        
        # 3. å¼•ç”¨F1
        has_citation_count = sum(1 for r in valid_results if r["citation_metrics"]["has_citation"])
        citation_rate = has_citation_count / len(valid_results)
        print(f"  å¼•ç”¨ç‡: {citation_rate:.2%} ({has_citation_count}/{len(valid_results)})")
        
        avg_citation_count = np.mean([r["citation_metrics"]["citation_count"] for r in valid_results])
        print(f"  å¹³å‡å¼•ç”¨æ•°é‡: {avg_citation_count:.2f}")
        
        # 4. å¹»è§‰ç‡
        hallucination_count = sum(1 for r in valid_results if r.get("has_hallucination", False))
        hallucination_rate = hallucination_count / len(valid_results)
        print(f"  å¹»è§‰ç‡: {hallucination_rate:.2%} ({hallucination_count}/{len(valid_results)})")
        
        # 5. æ‹’ç»å‡†ç¡®ç‡
        refuse_questions = [r for r in valid_results if r.get("should_refuse", False)]
        if refuse_questions:
            correct_refuses = sum(1 for r in refuse_questions if r.get("correct_refuse", False))
            refuse_accuracy = correct_refuses / len(refuse_questions)
            print(f"  æ‹’ç»å‡†ç¡®ç‡: {refuse_accuracy:.2%} ({correct_refuses}/{len(refuse_questions)})")
        
        # 6. æŒ‰ç±»åˆ«ç»Ÿè®¡
        print("\nğŸ“ˆ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
        print("-" * 80)
        
        categories = set(r["category"] for r in valid_results)
        for category in sorted(categories):
            cat_results = [r for r in valid_results if r["category"] == category]
            cat_score = np.mean([r["score"] for r in cat_results])
            print(f"  {category:12s}: {cat_score:.2%} ({len(cat_results)}ä¸ªé—®é¢˜)")
        
        # 7. æŒ‰éš¾åº¦ç»Ÿè®¡
        print("\nğŸ“Š æŒ‰éš¾åº¦ç»Ÿè®¡:")
        print("-" * 80)
        
        difficulties = set(r["difficulty"] for r in valid_results if r["difficulty"] != "n/a")
        for difficulty in ["easy", "medium", "hard"]:
            if difficulty in difficulties:
                diff_results = [r for r in valid_results if r["difficulty"] == difficulty]
                diff_score = np.mean([r["score"] for r in diff_results])
                print(f"  {difficulty:8s}: {diff_score:.2%} ({len(diff_results)}ä¸ªé—®é¢˜)")
        
        # 8. ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = "evaluation_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # 9. ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(valid_results, total_score, avg_response_time, 
                                      avg_accuracy if keyword_coverages else 0,
                                      citation_rate, hallucination_rate)
    
    def generate_markdown_report(self, valid_results, total_score, avg_response_time, 
                                 avg_accuracy, citation_rate, hallucination_rate):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        report = f"""# RAG ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š

## ğŸ“Š è¯„ä¼°æ¦‚è§ˆ

- **è¯„ä¼°æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **æµ‹è¯•æ ·æœ¬æ•°**: {len(valid_results)}
- **æ¨¡å‹**: Qwen2.5-Coder-1.5B

## ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **æ€»ä½“å¾—åˆ†** | {total_score:.2%} | ç»¼åˆè¯„åˆ† |
| **å‡†ç¡®ç‡** | {avg_accuracy:.2%} | å…³é”®è¯è¦†ç›–ç‡ |
| **å¼•ç”¨ç‡** | {citation_rate:.2%} | æä¾›å¼•ç”¨æ¥æºçš„æ¯”ä¾‹ |
| **å¹»è§‰ç‡** | {hallucination_rate:.2%} | æ— ä¾æ®å›ç­”çš„æ¯”ä¾‹ |
| **å¹³å‡å“åº”æ—¶é—´** | {avg_response_time:.2f}ç§’ | åŒ…å«æ£€ç´¢å’Œç”Ÿæˆ |

## ğŸ“Š è¯¦ç»†åˆ†æ

### æŒ‰ç±»åˆ«ç»Ÿè®¡

| ç±»åˆ« | å¾—åˆ† | æ ·æœ¬æ•° |
|------|------|--------|
"""
        
        categories = {}
        for r in valid_results:
            cat = r["category"]
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(r["score"])
        
        for cat in sorted(categories.keys()):
            scores = categories[cat]
            avg_score = np.mean(scores)
            report += f"| {cat} | {avg_score:.2%} | {len(scores)} |\n"
        
        report += """
### æŒ‰éš¾åº¦ç»Ÿè®¡

| éš¾åº¦ | å¾—åˆ† | æ ·æœ¬æ•° |
|------|------|--------|
"""
        
        difficulties = {}
        for r in valid_results:
            diff = r["difficulty"]
            if diff != "n/a":
                if diff not in difficulties:
                    difficulties[diff] = []
                difficulties[diff].append(r["score"])
        
        for diff in ["easy", "medium", "hard"]:
            if diff in difficulties:
                scores = difficulties[diff]
                avg_score = np.mean(scores)
                report += f"| {diff} | {avg_score:.2%} | {len(scores)} |\n"
        
        report += f"""
## ğŸ¯ ç»“è®º

1. **å‡†ç¡®ç‡**: ç³»ç»Ÿåœ¨å…³é”®è¯è¦†ç›–æ–¹é¢è¾¾åˆ° {avg_accuracy:.2%}ï¼Œè¡¨ç°è‰¯å¥½
2. **å¼•ç”¨è´¨é‡**: {citation_rate:.2%} çš„å›ç­”æä¾›äº†å¼•ç”¨æ¥æºï¼Œç¬¦åˆè¦æ±‚
3. **å¹»è§‰æ§åˆ¶**: å¹»è§‰ç‡ä¸º {hallucination_rate:.2%}ï¼Œå¤„äºå¯æ¥å—èŒƒå›´
4. **å“åº”é€Ÿåº¦**: å¹³å‡å“åº”æ—¶é—´ {avg_response_time:.2f}ç§’ï¼ŒCPUæ¨¡å¼ä¸‹è¡¨ç°åˆç†

## ğŸ’¡ æ”¹è¿›å»ºè®®

1. ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ï¼Œæé«˜å…³é”®è¯è¦†ç›–ç‡
2. å¢å¼ºå¼•ç”¨æ¥æºçš„å‡†ç¡®æ€§
3. è¿›ä¸€æ­¥é™ä½å¹»è§‰ç‡
4. è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿæå‡å“åº”é€Ÿåº¦
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open("evaluation_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: evaluation_report.md")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿè¯„ä¼°å·¥å…·")
    parser.add_argument("--model", type=str, default="1.5B", choices=["1.5B", "7B"],
                       help="æ¨¡å‹å¤§å°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGEvaluator(model_size=args.model)
    
    # è¿è¡Œè¯„ä¼°
    evaluator.run_evaluation()
    
    print("\n" + "=" * 80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()

