# RAG 系统评估指南

## 📊 评估概述

本评估脚本用于全面评估 RAG 系统的性能，包括：
- ✅ 准确率（关键词覆盖率）
- ✅ 引用F1（引用来源质量）
- ✅ 幻觉率（无依据回答比例）
- ✅ 响应时间
- ✅ 拒绝不确定回答的能力

## 🚀 快速开始

### 运行评估

```bash
# 使用 1.5B 模型评估（推荐）
python evaluate.py --model 1.5B

# 使用 7B 模型评估
python evaluate.py --model 7B
```

### 评估时间

- **1.5B 模型**: 约 10-15 分钟（22个测试样本）
- **7B 模型**: 约 20-30 分钟（22个测试样本）

## 📋 测试数据集

### 测试样本分布

| 类别 | 数量 | 说明 |
|------|------|------|
| **基础概念** | 10个 | Type-1/2/3/4克隆、AST、Token等 |
| **工具比较** | 5个 | NiCad、CCFinder、SourcererCC等 |
| **技术细节** | 5个 | 评估方法、性能优化等 |
| **不确定问题** | 2个 | 测试拒绝回答能力 |
| **总计** | **22个** | 覆盖多种场景 |

### 难度分布

- **简单 (easy)**: 7个
- **中等 (medium)**: 11个
- **困难 (hard)**: 4个

## 📊 评估指标

### 1. 准确率（Accuracy）

**定义**: 关键词覆盖率

**计算方法**:
```
准确率 = 匹配的关键词数量 / 预期关键词总数
```

**示例**:
- 问题: "什么是Type-1克隆？"
- 预期关键词: ["Type-1", "完全相同", "空格", "注释"]
- 回答包含: ["Type-1", "完全相同", "注释"]
- 准确率: 3/4 = 75%

### 2. 引用率（Citation Rate）

**定义**: 提供引用来源的回答比例

**计算方法**:
```
引用率 = 有引用的回答数 / 总回答数
```

**评分标准**:
- ✅ 优秀: >80%
- ⚠️ 良好: 60-80%
- ❌ 需改进: <60%

### 3. 幻觉率（Hallucination Rate）

**定义**: 无依据回答的比例

**检测规则**:
- 没有引用来源
- 回答长度 >100 字符
- 未明确说明"没有找到相关信息"

**计算方法**:
```
幻觉率 = 检测到幻觉的回答数 / 总回答数
```

**评分标准**:
- ✅ 优秀: <10%
- ⚠️ 良好: 10-20%
- ❌ 需改进: >20%

### 4. 响应时间（Response Time）

**定义**: 从提问到获得回答的时间

**包含**:
- 文档检索时间
- LLM 生成时间

**评分标准**（CPU模式）:
- ✅ 优秀: <20秒
- ⚠️ 良好: 20-35秒
- ❌ 需改进: >35秒

### 5. 拒绝准确率（Refusal Accuracy）

**定义**: 正确拒绝不确定问题的比例

**测试问题**:
- "明天天气怎么样？"
- "如何做红烧肉？"

**评分标准**:
- ✅ 正确拒绝: 包含"不确定"、"无法回答"等
- ❌ 错误回答: 给出了具体答案

## 📈 输出结果

### 1. 控制台输出

实时显示每个问题的评估结果：

```
[1/22] 测试问题: 什么是代码克隆检测？
   类别: concept | 难度: easy
   ✅ 得分: 0.85
   关键词覆盖: 80%
   引用数量: 3
   响应时间: 2.34秒
```

### 2. JSON 详细结果

保存到 `evaluation_results.json`:

```json
{
  "question": "什么是代码克隆检测？",
  "answer": "...",
  "sources": ["papers/clone_detection_basics.txt"],
  "score": 0.85,
  "quality_metrics": {
    "keyword_coverage": 0.8,
    "matched_keywords": ["代码克隆", "相似", "重复"],
    "length_score": 1.0,
    "answer_length": 256
  },
  "citation_metrics": {
    "has_citation": true,
    "citation_count": 3,
    "unique_sources": 2
  },
  "has_hallucination": false,
  "response_time": 2.34
}
```

### 3. Markdown 报告

保存到 `evaluation_report.md`:

- 评估概览
- 核心指标表格
- 按类别/难度统计
- 结论和改进建议

## 🎯 评分标准

### 综合得分计算

对于正常问题：
```
综合得分 = 关键词覆盖率 × 0.4
         + 长度合理性 × 0.2
         + 有引用来源 × 0.3
         + 无幻觉 × 0.1
```

对于不确定问题：
```
综合得分 = 1.0 (正确拒绝)
         = 0.0 (错误回答)
```

### 等级划分

| 得分范围 | 等级 | 说明 |
|---------|------|------|
| 90-100% | 优秀 | 回答准确、引用完整 |
| 80-90% | 良好 | 回答基本准确 |
| 70-80% | 中等 | 回答部分准确 |
| <70% | 需改进 | 回答不够准确 |

## 📊 预期结果

### 基线性能（1.5B 模型，CPU 模式）

| 指标 | 预期值 | 说明 |
|------|--------|------|
| 总体得分 | 75-85% | 综合评分 |
| 准确率 | 70-80% | 关键词覆盖 |
| 引用率 | 85-95% | 提供引用 |
| 幻觉率 | 5-15% | 控制幻觉 |
| 响应时间 | 20-35秒 | CPU 模式 |
| 拒绝准确率 | 90-100% | 正确拒绝 |

### 优化后性能（7B 模型，GPU 模式）

| 指标 | 预期值 | 改进 |
|------|--------|------|
| 总体得分 | 85-95% | +10% |
| 准确率 | 80-90% | +10% |
| 引用率 | 90-95% | +5% |
| 幻觉率 | 3-10% | -5% |
| 响应时间 | 2-5秒 | -85% |
| 拒绝准确率 | 95-100% | +5% |

## 🔧 自定义评估

### 添加测试问题

编辑 `evaluate.py` 中的 `_load_test_data()` 方法：

```python
{
    "question": "你的问题",
    "expected_keywords": ["关键词1", "关键词2"],
    "category": "concept",  # concept/tool/technical/uncertain
    "difficulty": "medium"  # easy/medium/hard
}
```

### 调整评分权重

修改 `run_evaluation()` 方法中的权重：

```python
score = (
    quality_metrics["keyword_coverage"] * 0.4 +  # 关键词权重
    quality_metrics["length_score"] * 0.2 +      # 长度权重
    (1.0 if citation_metrics["has_citation"] else 0.0) * 0.3 +  # 引用权重
    (0.0 if has_hallucination else 1.0) * 0.1   # 幻觉权重
)
```

## 💡 使用建议

### 1. 首次评估

```bash
# 使用轻量级模型快速评估
python evaluate.py --model 1.5B
```

### 2. 查看结果

```bash
# 查看详细结果
cat evaluation_results.json

# 查看报告
cat evaluation_report.md
```

### 3. 对比评估

```bash
# 评估基线版本
python evaluate.py --model 1.5B > baseline_results.txt

# 优化后再次评估
python evaluate.py --model 1.5B > optimized_results.txt

# 对比结果
diff baseline_results.txt optimized_results.txt
```

## 📝 注意事项

1. **首次运行**: 需要先加载模型和向量数据库，可能需要几分钟
2. **CPU 模式**: 响应时间较长，请耐心等待
3. **测试样本**: 可以根据需要增加或修改测试问题
4. **评分标准**: 可以根据实际情况调整权重

## 🎓 评估报告用途

1. **作业提交**: 将 `evaluation_report.md` 添加到实验报告中
2. **性能分析**: 识别系统的优势和不足
3. **迭代优化**: 指导后续的优化方向
4. **对比实验**: 比较不同配置的性能

---

**提示**: 评估完成后，将结果添加到实验报告的"实验结果"部分。

