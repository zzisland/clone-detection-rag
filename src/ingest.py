import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any
import PyPDF2
from bs4 import BeautifulSoup
import markdown
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import sys
sys.path.insert(0, str(Path(__file__).parent))
from config import Config

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼Œè´Ÿè´£è¯»å–å’Œæ¸…æ´—å„ç§æ ¼å¼çš„æ–‡æ¡£"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def read_file(self, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ–‡ä»¶å†…å®¹"""
        file_ext = Path(file_path).suffix.lower()
        
        try:
            if file_ext == '.txt':
                return self._read_txt(file_path)
            elif file_ext == '.md':
                return self._read_markdown(file_path)
            elif file_ext == '.pdf':
                return self._read_pdf(file_path)
            elif file_ext == '.html':
                return self._read_html(file_path)
            elif file_ext in ['.py', '.js', '.java', '.cpp', '.c']:
                return self._read_code(file_path)
            else:
                print(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
                return ""
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return ""
    
    def _read_txt(self, file_path: str) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _read_markdown(self, file_path: str) -> str:
        """è¯»å–Markdownæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # è½¬æ¢ä¸ºHTMLç„¶åæå–çº¯æ–‡æœ¬
            html = markdown.markdown(content)
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text()
    
    def _read_pdf(self, file_path: str) -> str:
        """è¯»å–PDFæ–‡ä»¶"""
        text = ""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    
    def _read_html(self, file_path: str) -> str:
        """è¯»å–HTMLæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            return soup.get_text()
    
    def _read_code(self, file_path: str) -> str:
        """è¯»å–ä»£ç æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()[\]{}"\'-]', ' ', text)
        # ç§»é™¤è¿‡çŸ­çš„è¡Œ
        lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 10]
        return '\n'.join(lines)
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        return self.text_splitter.split_documents(documents)

class DataIngestor:
    """æ•°æ®æ‘„å–å™¨ï¼Œè´Ÿè´£å¤„ç†æ•´ä¸ªæ•°æ®æ‘„å–æµç¨‹"""
    
    def __init__(self):
        self.processor = DocumentProcessor()
        # ä½¿ç”¨ HuggingFace çš„ä¸­æ–‡ Embedding æ¨¡å‹
        import torch
        # ä¸´æ—¶å¼ºåˆ¶ä½¿ç”¨ CPU
        device = 'cpu'
        print(f"âš ï¸ Embedding ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆRTX 5060 éœ€è¦æ›´æ–°çš„ PyTorchï¼‰")
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5",  # ä¸­æ–‡å‘é‡æ¨¡å‹ï¼Œè½»é‡é«˜æ•ˆ
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        print(f"Embedding æ¨¡å‹åŠ è½½å®Œæˆ: BAAI/bge-small-zh-v1.5 (è®¾å¤‡: {device})")
    
    def load_documents_from_directory(self, directory: str) -> List[Document]:
        """ä»ç›®å½•åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        documents = []
        directory_path = Path(directory)
        
        if not directory_path.exists():
            print(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
            return documents
        
        # é¦–å…ˆå¤„ç†æ™®é€šæ–‡æ¡£æ–‡ä»¶
        for file_path in directory_path.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in Config.SUPPORTED_EXTENSIONS:
                print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
                content = self.processor.read_file(str(file_path))
                if content:
                    cleaned_content = self.processor.clean_text(content)
                    metadata = {
                        'source': str(file_path),
                        'file_type': file_path.suffix.lower(),
                        'file_name': file_path.name,
                        'directory': file_path.parent.name
                    }
                    doc = Document(page_content=cleaned_content, metadata=metadata)
                    documents.append(doc)
        
        # å¤„ç†ç”¨æˆ·æä¾›çš„è®ºæ–‡æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        processed_dir = Path("data/google_scholar_papers")
        if processed_dir.exists():
            documents.extend(self._load_enhanced_dataset(processed_dir))
        
        return documents
    
    def _load_enhanced_dataset(self, enhanced_dir: Path) -> List[Document]:
        """åŠ è½½å¢å¼ºæ•°æ®é›†"""
        documents = []
        
        # å¤„ç†è®ºæ–‡æ–‡æœ¬æ–‡ä»¶
        texts_dir = enhanced_dir / "texts"
        if texts_dir.exists():
            for file_path in texts_dir.rglob('*.txt'):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if content:
                        cleaned_content = self.processor.clean_text(content)
                        metadata = {
                            'source': str(file_path),
                            'file_type': '.txt',
                            'file_name': file_path.name,
                            'directory': enhanced_dir.name,
                            'content_type': 'paper'
                        }
                        doc = Document(page_content=cleaned_content, metadata=metadata)
                        documents.append(doc)
                        print(f"åŠ è½½å¢å¼ºè®ºæ–‡: {file_path.name}")
                except Exception as e:
                    print(f"åŠ è½½å¢å¼ºè®ºæ–‡å¤±è´¥ {file_path}: {e}")
        
        # å¤„ç†é—®ç­”å¯¹
        qa_file = enhanced_dir / "qa_pairs.json"
        if qa_file.exists():
            try:
                with open(qa_file, 'r', encoding='utf-8') as f:
                    qa_pairs = json.load(f)
                
                for i, qa_pair in enumerate(qa_pairs):
                    question = qa_pair.get('question', '')
                    answer = qa_pair.get('answer', '')
                    source = qa_pair.get('source', '')
                    qa_type = qa_pair.get('type', 'unknown')
                    
                    # å°†é—®ç­”å¯¹è½¬æ¢ä¸ºæ–‡æ¡£æ ¼å¼
                    qa_content = f"é—®é¢˜: {question}\nç­”æ¡ˆ: {answer}"
                    cleaned_content = self.processor.clean_text(qa_content)
                    
                    metadata = {
                        'source': f"qa_pairs_{i+1}",
                        'file_type': '.json',
                        'file_name': f"qa_pair_{i+1}.json",
                        'directory': enhanced_dir.name,
                        'content_type': 'qa_pair',
                        'qa_type': qa_type,
                        'original_source': source
                    }
                    
                    doc = Document(page_content=cleaned_content, metadata=metadata)
                    documents.append(doc)
                
                print(f"åŠ è½½ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            except Exception as e:
                print(f"åŠ è½½é—®ç­”å¯¹å¤±è´¥: {e}")
        
        return documents
    
    def create_vector_store(self, documents: List[Document]) -> Chroma:
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        if not documents:
            print("æ²¡æœ‰æ–‡æ¡£å¯ä»¥å¤„ç†")
            return None
        
        print(f"æ­£åœ¨å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        split_docs = self.processor.split_documents(documents)
        print(f"åˆ†å‰²åå¾—åˆ° {len(split_docs)} ä¸ªæ–‡æ¡£å—")
        
        # æ‰¹é‡å¤„ç†å‘é‡åŒ–
        print("å¼€å§‹å‘é‡åŒ–å¤„ç†...")
        batch_size = 100  # æ‰¹é‡å¤§å°
        
        for i in range(0, len(split_docs), batch_size):
            batch = split_docs[i:i+batch_size]
            print(f"å¤„ç†è¿›åº¦: {i+batch_size}/{len(split_docs)} ({(i+batch_size)/len(split_docs)*100:.1f}%)")
            
            if i == 0:
                # ç¬¬ä¸€æ‰¹åˆ›å»ºå‘é‡æ•°æ®åº“
                vector_store = Chroma.from_documents(
                    documents=batch,
                    embedding=self.embeddings,
                    persist_directory=Config.CHROMA_PERSIST_DIRECTORY
                )
            else:
                # åç»­æ‰¹æ¬¡æ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“
                vector_store.add_documents(batch)
        
        # æŒä¹…åŒ–
        vector_store.persist()
        print(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {Config.CHROMA_PERSIST_DIRECTORY}")
        
        return vector_store
    
    def ingest_all_data(self, force_refresh: bool = False) -> Chroma:
        """æ‘„å–æ‰€æœ‰æ•°æ®ç›®å½•ä¸­çš„æ–‡æ¡£"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡æ•°æ®åº“
        if not force_refresh and os.path.exists(Config.CHROMA_PERSIST_DIRECTORY):
            try:
                # å°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“
                vector_store = Chroma(
                    persist_directory=Config.CHROMA_PERSIST_DIRECTORY,
                    embedding_function=self.embeddings
                )
                print("âœ… å·²åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œè·³è¿‡æ•°æ®æ‘„å–")
                print("ğŸ’¡ å¦‚éœ€é‡æ–°æ‘„å–ï¼Œè¯·ä½¿ç”¨ force_refresh=True")
                return vector_store
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç°æœ‰æ•°æ®åº“å¤±è´¥: {e}")
                print("ğŸ”„ é‡æ–°è¿›è¡Œæ•°æ®æ‘„å–...")
        
        all_documents = []
        
        for dir_name, dir_path in Config.DATA_DIRS.items():
            print(f"\næ­£åœ¨å¤„ç†ç›®å½•: {dir_name}")
            documents = self.load_documents_from_directory(dir_path)
            all_documents.extend(documents)
            print(f"ä» {dir_name} åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        if all_documents:
            return self.create_vector_store(all_documents)
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return None

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ•°æ®æ‘„å–åŠŸèƒ½"""
    ingestor = DataIngestor()
    vector_store = ingestor.ingest_all_data()
    
    if vector_store:
        print("æ•°æ®æ‘„å–å®Œæˆ!")
        # æµ‹è¯•æ£€ç´¢
        query = "ä»€ä¹ˆæ˜¯ä»£ç å…‹éš†æ£€æµ‹?"
        docs = vector_store.similarity_search(query, k=3)
        print(f"\næµ‹è¯•æ£€ç´¢ '{query}' çš„ç»“æœ:")
        for i, doc in enumerate(docs, 1):
            print(f"\næ–‡æ¡£ {i}:")
            print(f"æ¥æº: {doc.metadata.get('source', 'Unknown')}")
            print(f"å†…å®¹: {doc.page_content[:200]}...")

if __name__ == "__main__":
    main()

